# Pairs-and-Stripes-in-Hadoop-and-Spark
Comparing Stripes and Pairs approach

I have written a python script that takes in the ratings file and generates a file where each line is a list of movies that were rated highly by a user. So, each line corresponds one user. Number of lines in this files the number of users. Also, the movie IDs in a line are sorted in increasing order. This helps me in maintaining the lexicographic order while counting.
All my scripts here use the files generated by the script above to calculate the pair count, conditional probability and lift.

While generating the stripes in my programs, I use the following approach. 
Let's say, the program encounters a line like "2 4 6 8" which are the movie IDs. My program generates stripes like "2 4 6 8", "4 6 8" and "6 8".
I then use python counters to maintain the count for each movie.

Performed the following tasks:
1. Wrote the MapReduce programs in hadoop and Spark to compute the frequency of co-occurrence for every pair of movies that receive a "High" ranking (4-5 star rating) from the same user using both Pairs and Stripes approach.

2. Computed the conditional probability of P(MovieB | MovieA) and emitted movie names in pairs whose conditional probability is greater than 0.8. This approach is used as a primitive way to recommend movies to customers that liked a certain movie. So, if a user liked movieA and rated it higher, then there there is more than 80% chance that he will like the movieB and rate it high.

3. Computed the Lift between two movies and emitted pairs whose lift is greater than 1.6. 
Lift is a measure of the effectiveness of a predictive model (or association rule) calculated as the ratio between the results obtained with and without the predictive model.

--All the tasks in the submission were implemented in Python, executed using MapReduce on Hadoop using Hadoop Streaming and PySpark in Spark. Used all the datasets namely MovieLens 100K, 200K, 500K, MovieLens 1M and MovieLens 10M to perform the below mentioned tasks. Recorded the running times for various file sizes. Then, created graphs for each task showing running time vs the data size.

1. The zipped file contains one folder which contains 3 folders : Code, Pseuodocode  and Output
2. The output folder contains the output files and the graphs
3. The folder named "Code" has 4 subfolders
    a. The folder Paircount_Hadoop contains python scripts for pairs and stripes for the movie pair count in Hadoop
    b. The folder Paircount_Spark contains python scripts for pairs and stripes for the movie pair count in Spark
    c. The folder Conditional_Probability_Spark contains the python script for calculating the conditional probability in Spark using stripes approach
    d. The folder Lift_Spark contains the python script to calculate lift in Spark using stripes approach
    e. The folder Extra contains python scripts that generate the input data that are  used by all the programs. It also has a python script that prints the movie names in the sample output submitted. My programs output movie IDs. I used this python script to print the movie names.
3. The folder named "Pseudocode" has 4 subfolders
    a. The folder Paircount_Hadoop contains pseudocode for pairs and stripes for the movie pair count in Hadoop
    b. The folder Paircount_Spark contains pseudocode for pairs and stripes for the movie pair count in Spark
    c. The folder Conditional_Probability_Spark contains the pseudocode for the python script calculating the conditional probability in spark
    d. The folder Lift_Spark contains the pseudocode to calculate lift in Spark

Conclusion: According to the graphs plotted for the runtimes for hadoop and spark for both pairs and stripes approach, it can be seen that Spark has better times in comparison to the Hadoop and stripes approach is much faster and more efficient than the pairs.
